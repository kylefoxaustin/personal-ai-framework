# Personal AI Framework Configuration

model:
  base_model: "mixtral-8x7b-instruct"
  quantization: "Q4_K_M"  # 4-bit quantization
  context_length: 32768
  max_new_tokens: 2048
  
personalization:
  lora_rank: 64
  lora_alpha: 128
  training_epochs: 3
  learning_rate: 1e-4

vectordb:
  type: "chromadb"
  embedding_model: "BAAI/bge-large-en-v1.5"
  chunk_size: 512
  chunk_overlap: 50

pipeline:
  retrieval_k: 10  # Top K documents to retrieve
  rerank_k: 3      # Final documents after reranking
  
server:
  host: "0.0.0.0"
  port: 8080
  workers: 4
