services:
  llm-server:
    build:
      context: .
      dockerfile: docker/Dockerfile.llm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/app/models
      - ./pipeline:/app
      - ./pipeline/config.yaml:/app/config.yaml:ro
    ports:
      - "8080:8080"

  vectordb:
    image: chromadb/chroma:0.5.0
    volumes:
      - ./vectordb:/data
    ports:
      - "8000:8000"
