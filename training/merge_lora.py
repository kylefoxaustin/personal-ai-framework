#!/usr/bin/env python3
"""Merge LoRA adapters into base model for deployment."""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

MODEL_PATH = "mistralai/Mistral-7B-Instruct-v0.2"
LORA_PATH = "training/output/final"
OUTPUT_PATH = "training/output/merged"

print("="*60)
print("Merging LoRA adapters into base model")
print("="*60)

print("\nðŸ“¥ Loading base model (this needs ~30GB RAM)...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="cpu",  # Load to CPU for merging
    low_cpu_mem_usage=True,
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

print("ðŸ“¥ Loading LoRA adapters...")
model = PeftModel.from_pretrained(model, LORA_PATH, device_map="cpu")

print("ðŸ”§ Merging weights...")
model = model.merge_and_unload()

print(f"ðŸ’¾ Saving merged model to {OUTPUT_PATH}...")
os.makedirs(OUTPUT_PATH, exist_ok=True)
model.save_pretrained(OUTPUT_PATH, safe_serialization=True)
tokenizer.save_pretrained(OUTPUT_PATH)

print("\nâœ… Merged model saved!")
print(f"Size: {sum(f.stat().st_size for f in __import__('pathlib').Path(OUTPUT_PATH).rglob('*') if f.is_file()) / 1e9:.1f} GB")
print("\nNext step: Convert to GGUF format")
